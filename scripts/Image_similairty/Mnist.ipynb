{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "import json\n",
    "import logging\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"General utility functions\"\"\"\n",
    "class Params():\n",
    "    \"\"\"Class that loads hyperparameters from a json file.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    params = Params(json_path)\n",
    "    print(params.learning_rate)\n",
    "    params.learning_rate = 0.5  # change the value of learning_rate in params\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, json_path):\n",
    "        self.update(json_path)\n",
    "\n",
    "    def save(self, json_path):\n",
    "        \"\"\"Saves parameters to json file\"\"\"\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(self.__dict__, f, indent=4)\n",
    "\n",
    "    def update(self, json_path):\n",
    "        \"\"\"Loads parameters from json file\"\"\"\n",
    "        with open(json_path) as f:\n",
    "            params = json.load(f)\n",
    "            self.__dict__.update(params)\n",
    "\n",
    "    @property\n",
    "    def dict(self):\n",
    "        \"\"\"Gives dict-like access to Params instance by `params.dict['learning_rate']`\"\"\"\n",
    "        return self.__dict__\n",
    "\n",
    "\n",
    "def set_logger(log_path):\n",
    "    \"\"\"Sets the logger to log info in terminal and file `log_path`.\n",
    "\n",
    "    In general, it is useful to have a logger so that every output to the terminal is saved\n",
    "    in a permanent file. Here we save it to `model_dir/train.log`.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    logging.info(\"Starting training...\")\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        log_path: (string) where to log\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        # Logging to a file\n",
    "        file_handler = logging.FileHandler(log_path)\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "        # Logging to console\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        stream_handler.setFormatter(logging.Formatter('%(message)s'))\n",
    "        logger.addHandler(stream_handler)\n",
    "\n",
    "\n",
    "def save_dict_to_json(d, json_path):\n",
    "    \"\"\"Saves dict of floats in json file\n",
    "\n",
    "    Args:\n",
    "        d: (dict) of float-castable values (np.float, int, float, etc.)\n",
    "        json_path: (string) path to json file\n",
    "    \"\"\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        # We need to convert the values to float for json (it doesn't accept np.array, np.float, )\n",
    "        d = {k: float(v) for k, v in d.items()}\n",
    "        json.dump(d, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define functions to create the triplet loss with online triplet mining.\"\"\"\n",
    "def _pairwise_distances(embeddings, squared=False):\n",
    "    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        pairwise_distances: tensor of shape (batch_size, batch_size)\n",
    "    \"\"\"\n",
    "    # Get the dot product between all embeddings\n",
    "    # shape (batch_size, batch_size)\n",
    "    dot_product = tf.matmul(embeddings, tf.transpose(embeddings))\n",
    "\n",
    "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
    "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
    "    # shape (batch_size,)\n",
    "    square_norm = tf.diag_part(dot_product)\n",
    "\n",
    "    # Compute the pairwise distance matrix as we have:\n",
    "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
    "    # shape (batch_size, batch_size)\n",
    "    distances = tf.expand_dims(square_norm, 1) - 2.0 * dot_product + tf.expand_dims(square_norm, 0)\n",
    "\n",
    "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
    "    distances = tf.maximum(distances, 0.0)\n",
    "\n",
    "    if not squared:\n",
    "        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
    "        # we need to add a small epsilon where distances == 0.0\n",
    "        mask = tf.to_float(tf.equal(distances, 0.0))\n",
    "        distances = distances + mask * 1e-16\n",
    "\n",
    "        distances = tf.sqrt(distances)\n",
    "\n",
    "        # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n",
    "        distances = distances * (1.0 - mask)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def _get_anchor_positive_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n",
    "\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i and j are distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "\n",
    "    # Check if labels[i] == labels[j]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(indices_not_equal, labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
    "\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check if labels[i] != labels[k]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "\n",
    "    mask = tf.logical_not(labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_triplet_mask(labels):\n",
    "    \"\"\"Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.\n",
    "\n",
    "    A triplet (i, j, k) is valid if:\n",
    "        - i, j, k are distinct\n",
    "        - labels[i] == labels[j] and labels[i] != labels[k]\n",
    "\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i, j and k are distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    i_not_equal_j = tf.expand_dims(indices_not_equal, 2)\n",
    "    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n",
    "    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n",
    "\n",
    "    distinct_indices = tf.logical_and(tf.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "\n",
    "\n",
    "    # Check if labels[i] == labels[j] and labels[i] != labels[k]\n",
    "    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
    "    i_equal_k = tf.expand_dims(label_equal, 1)\n",
    "\n",
    "    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(distinct_indices, valid_labels)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def batch_all_triplet_loss(labels, embeddings, margin, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    We generate all the valid triplets and average the loss over the positive ones.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    # Get the pairwise distance matrix\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    # shape (batch_size, batch_size, 1)\n",
    "    anchor_positive_dist = tf.expand_dims(pairwise_dist, 2)\n",
    "    assert anchor_positive_dist.shape[2] == 1, \"{}\".format(anchor_positive_dist.shape)\n",
    "    # shape (batch_size, 1, batch_size)\n",
    "    anchor_negative_dist = tf.expand_dims(pairwise_dist, 1)\n",
    "    assert anchor_negative_dist.shape[1] == 1, \"{}\".format(anchor_negative_dist.shape)\n",
    "\n",
    "    # Compute a 3D tensor of size (batch_size, batch_size, batch_size)\n",
    "    # triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k\n",
    "    # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)\n",
    "    # and the 2nd (batch_size, 1, batch_size)\n",
    "    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin\n",
    "\n",
    "    # Put to zero the invalid triplets\n",
    "    # (where label(a) != label(p) or label(n) == label(a) or a == p)\n",
    "    mask = _get_triplet_mask(labels)\n",
    "    mask = tf.to_float(mask)\n",
    "    triplet_loss = tf.multiply(mask, triplet_loss)\n",
    "\n",
    "    # Remove negative losses (i.e. the easy triplets)\n",
    "    triplet_loss = tf.maximum(triplet_loss, 0.0)\n",
    "\n",
    "    # Count number of positive triplets (where triplet_loss > 0)\n",
    "    valid_triplets = tf.to_float(tf.greater(triplet_loss, 1e-16))\n",
    "    num_positive_triplets = tf.reduce_sum(valid_triplets)\n",
    "    num_valid_triplets = tf.reduce_sum(mask)\n",
    "    fraction_positive_triplets = num_positive_triplets / (num_valid_triplets + 1e-16)\n",
    "\n",
    "    # Get final mean triplet loss over the positive valid triplets\n",
    "    triplet_loss = tf.reduce_sum(triplet_loss) / (num_positive_triplets + 1e-16)\n",
    "\n",
    "    return triplet_loss, fraction_positive_triplets\n",
    "\n",
    "\n",
    "def batch_hard_triplet_loss(labels, embeddings, margin, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    # Get the pairwise distance matrix\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    # For each anchor, get the hardest positive\n",
    "    # First, we need to get a mask for every valid positive (they should have same label)\n",
    "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n",
    "    mask_anchor_positive = tf.to_float(mask_anchor_positive)\n",
    "\n",
    "    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n",
    "    anchor_positive_dist = tf.multiply(mask_anchor_positive, pairwise_dist)\n",
    "\n",
    "    # shape (batch_size, 1)\n",
    "    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=1, keepdims=True)\n",
    "    tf.summary.scalar(\"hardest_positive_dist\", tf.reduce_mean(hardest_positive_dist))\n",
    "\n",
    "    # For each anchor, get the hardest negative\n",
    "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
    "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
    "    mask_anchor_negative = tf.to_float(mask_anchor_negative)\n",
    "\n",
    "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
    "    max_anchor_negative_dist = tf.reduce_max(pairwise_dist, axis=1, keepdims=True)\n",
    "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
    "\n",
    "    # shape (batch_size,)\n",
    "    hardest_negative_dist = tf.reduce_min(anchor_negative_dist, axis=1, keepdims=True)\n",
    "    tf.summary.scalar(\"hardest_negative_dist\", tf.reduce_mean(hardest_negative_dist))\n",
    "\n",
    "    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
    "    triplet_loss = tf.maximum(hardest_positive_dist - hardest_negative_dist + margin, 0.0)\n",
    "\n",
    "    # Get final mean triplet loss\n",
    "    triplet_loss = tf.reduce_mean(triplet_loss)\n",
    "\n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define the model.\"\"\"\n",
    "def build_model(is_training, images, params):\n",
    "    \"\"\"Compute outputs of the model (embeddings for triplet loss).\n",
    "\n",
    "    Args:\n",
    "        is_training: (bool) whether we are training or not\n",
    "        images: (dict) contains the inputs of the graph (features)\n",
    "                this can be `tf.placeholder` or outputs of `tf.data`\n",
    "        params: (Params) hyperparameters\n",
    "\n",
    "    Returns:\n",
    "        output: (tf.Tensor) output of the model\n",
    "    \"\"\"\n",
    "    out = images\n",
    "    # Define the number of channels of each convolution\n",
    "    # For each block, we do: 3x3 conv -> batch norm -> relu -> 2x2 maxpool\n",
    "    num_channels = params.num_channels\n",
    "    bn_momentum = params.bn_momentum\n",
    "    channels = [num_channels, num_channels * 2]\n",
    "    for i, c in enumerate(channels):\n",
    "        with tf.variable_scope('block_{}'.format(i+1)):\n",
    "            out = tf.layers.conv2d(out, c, 3, padding='same')\n",
    "            if params.use_batch_norm:\n",
    "                out = tf.layers.batch_normalization(out, momentum=bn_momentum, training=is_training)\n",
    "            out = tf.nn.relu(out)\n",
    "            out = tf.layers.max_pooling2d(out, 2, 2)\n",
    "\n",
    "    assert out.shape[1:] == [7, 7, num_channels * 2]\n",
    "\n",
    "    out = tf.reshape(out, [-1, 7 * 7 * num_channels * 2])\n",
    "    with tf.variable_scope('fc_1'):\n",
    "        out = tf.layers.dense(out, params.embedding_size)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    \"\"\"Model function for tf.estimator\n",
    "\n",
    "    Args:\n",
    "        features: input batch of images\n",
    "        labels: labels of the images\n",
    "        mode: can be one of tf.estimator.ModeKeys.{TRAIN, EVAL, PREDICT}\n",
    "        params: contains hyperparameters of the model (ex: `params.learning_rate`)\n",
    "\n",
    "    Returns:\n",
    "        model_spec: tf.estimator.EstimatorSpec object\n",
    "    \"\"\"\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    images = features[\"x\"]\n",
    "    images = tf.reshape(images, [-1, params.image_size, params.image_size, 1])\n",
    "    assert images.shape[1:] == [params.image_size, params.image_size, 1], \"{}\".format(images.shape)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # MODEL: define the layers of the model\n",
    "    with tf.variable_scope('model'):\n",
    "        # Compute the embeddings with the model\n",
    "        embeddings = build_model(is_training, images, params)\n",
    "    embedding_mean_norm = tf.reduce_mean(tf.norm(embeddings, axis=1))\n",
    "    tf.summary.scalar(\"embedding_mean_norm\", embedding_mean_norm)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {'embeddings': embeddings}\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "\n",
    "    # Define triplet loss\n",
    "    if params.triplet_strategy == \"batch_all\":\n",
    "        loss, fraction = batch_all_triplet_loss(labels, embeddings, margin=params.margin,\n",
    "                                                squared=params.squared)\n",
    "    elif params.triplet_strategy == \"batch_hard\":\n",
    "        loss = batch_hard_triplet_loss(labels, embeddings, margin=params.margin,\n",
    "                                       squared=params.squared)\n",
    "    else:\n",
    "        raise ValueError(\"Triplet strategy not recognized: {}\".format(params.triplet_strategy))\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # METRICS AND SUMMARIES\n",
    "    # Metrics for evaluation using tf.metrics (average over whole dataset)\n",
    "    # TODO: some other metrics like rank-1 accuracy?\n",
    "    with tf.variable_scope(\"metrics\"):\n",
    "        eval_metric_ops = {\"embedding_mean_norm\": tf.metrics.mean(embedding_mean_norm)}\n",
    "\n",
    "        if params.triplet_strategy == \"batch_all\":\n",
    "            eval_metric_ops['fraction_positive_triplets'] = tf.metrics.mean(fraction)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "    # Summaries for training\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    if params.triplet_strategy == \"batch_all\":\n",
    "        tf.summary.scalar('fraction_positive_triplets', fraction)\n",
    "\n",
    "    tf.summary.image('train_image', images, max_outputs=1)\n",
    "\n",
    "    # Define training step that minimizes the loss with the Adam optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(params.learning_rate)\n",
    "    global_step = tf.train.get_global_step()\n",
    "    if params.use_batch_norm:\n",
    "        # Add a dependency to update the moving mean and variance for batch normalization\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    else:\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 40s 4us/step\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(10, kernel_initializer='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 10s - loss: 0.2829 - acc: 0.9193 - val_loss: 0.1388 - val_acc: 0.9588\n",
      "Epoch 2/10\n",
      " - 4s - loss: 0.1095 - acc: 0.9685 - val_loss: 0.0943 - val_acc: 0.9719\n",
      "Epoch 3/10\n",
      " - 4s - loss: 0.0700 - acc: 0.9795 - val_loss: 0.0806 - val_acc: 0.9747\n",
      "Epoch 4/10\n",
      " - 4s - loss: 0.0489 - acc: 0.9863 - val_loss: 0.0684 - val_acc: 0.9785\n",
      "Epoch 5/10\n",
      " - 4s - loss: 0.0351 - acc: 0.9902 - val_loss: 0.0646 - val_acc: 0.9790\n",
      "Epoch 6/10\n",
      " - 4s - loss: 0.0264 - acc: 0.9929 - val_loss: 0.0619 - val_acc: 0.9810\n",
      "Epoch 7/10\n",
      " - 4s - loss: 0.0193 - acc: 0.9952 - val_loss: 0.0611 - val_acc: 0.9803\n",
      "Epoch 8/10\n",
      " - 4s - loss: 0.0145 - acc: 0.9966 - val_loss: 0.0569 - val_acc: 0.9826\n",
      "Epoch 9/10\n",
      " - 4s - loss: 0.0100 - acc: 0.9981 - val_loss: 0.0586 - val_acc: 0.9817\n",
      "Epoch 10/10\n",
      " - 4s - loss: 0.0083 - acc: 0.9983 - val_loss: 0.0577 - val_acc: 0.9812\n",
      "Baseline Error: 1.88%\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "train_data = mnist.train.images # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 3, 4, ..., 5, 6, 8])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating the model...\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:/Retinal_Datasets/models/', '_tf_random_seed': 230, '_save_summary_steps': 50, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001CA665940F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Starting training for 20 epoch(s).\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:/Retinal_Datasets/models/model.ckpt-17188\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 17189 into D:/Retinal_Datasets/models/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.6391748, step = 17188\n",
      "INFO:tensorflow:global_step/sec: 46.3579\n",
      "INFO:tensorflow:loss = 0.22200164, step = 17288 (2.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6328\n",
      "INFO:tensorflow:loss = 0.0, step = 17388 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5413\n",
      "INFO:tensorflow:loss = 0.64756054, step = 17488 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7434\n",
      "INFO:tensorflow:loss = 0.0, step = 17588 (1.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.9359\n",
      "INFO:tensorflow:loss = 0.88029337, step = 17688 (1.889 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.9313\n",
      "INFO:tensorflow:loss = 0.609105, step = 17788 (1.890 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.9301\n",
      "INFO:tensorflow:loss = 0.9985512, step = 17888 (1.889 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.8212\n",
      "INFO:tensorflow:loss = 0.107524075, step = 17988 (1.893 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7763\n",
      "INFO:tensorflow:loss = 0.0, step = 18088 (1.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.8848\n",
      "INFO:tensorflow:loss = 0.81147665, step = 18188 (1.891 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7923\n",
      "INFO:tensorflow:loss = 0.50027406, step = 18288 (1.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.9643\n",
      "INFO:tensorflow:loss = 1.0209957, step = 18388 (2.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7661\n",
      "INFO:tensorflow:loss = 0.66238046, step = 18488 (2.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.5359\n",
      "INFO:tensorflow:loss = 0.0, step = 18588 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.8212\n",
      "INFO:tensorflow:loss = 0.9481034, step = 18688 (1.892 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.8431\n",
      "INFO:tensorflow:loss = 0.29923916, step = 18788 (1.893 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.8263\n",
      "INFO:tensorflow:loss = 1.5142504, step = 18888 (1.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6557\n",
      "INFO:tensorflow:loss = 0.22528887, step = 18988 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7994\n",
      "INFO:tensorflow:loss = 0.6713302, step = 19088 (1.893 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7082\n",
      "INFO:tensorflow:loss = 0.31384596, step = 19188 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.9691\n",
      "INFO:tensorflow:loss = 0.0, step = 19288 (1.887 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.9179\n",
      "INFO:tensorflow:loss = 0.78570783, step = 19388 (1.890 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.742\n",
      "INFO:tensorflow:loss = 0.4390545, step = 19488 (1.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.8502\n",
      "INFO:tensorflow:loss = 0.822014, step = 19588 (1.893 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.872\n",
      "INFO:tensorflow:loss = 1.9279023, step = 19688 (1.891 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.717\n",
      "INFO:tensorflow:loss = 0.4697517, step = 19788 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.8763\n",
      "INFO:tensorflow:loss = 1.0187639, step = 19888 (1.892 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7716\n",
      "INFO:tensorflow:loss = 0.5071273, step = 19988 (2.197 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.3434\n",
      "INFO:tensorflow:loss = 0.9320682, step = 20088 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.685\n",
      "INFO:tensorflow:loss = 1.0337697, step = 20188 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7612\n",
      "INFO:tensorflow:loss = 0.2598195, step = 20288 (1.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.64\n",
      "INFO:tensorflow:loss = 0.9080397, step = 20388 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6384\n",
      "INFO:tensorflow:loss = 1.3249671, step = 20488 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7306\n",
      "INFO:tensorflow:loss = 0.5969346, step = 20588 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5502\n",
      "INFO:tensorflow:loss = 0.109820366, step = 20688 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6306\n",
      "INFO:tensorflow:loss = 0.5087122, step = 20788 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6099\n",
      "INFO:tensorflow:loss = 0.17080624, step = 20888 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5806\n",
      "INFO:tensorflow:loss = 0.8184576, step = 20988 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.4075\n",
      "INFO:tensorflow:loss = 1.0539641, step = 21088 (2.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6669\n",
      "INFO:tensorflow:loss = 0.47813225, step = 21188 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6718\n",
      "INFO:tensorflow:loss = 0.4085107, step = 21288 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6569\n",
      "INFO:tensorflow:loss = 0.62344956, step = 21388 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.539\n",
      "INFO:tensorflow:loss = 0.057863235, step = 21488 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7089\n",
      "INFO:tensorflow:loss = 1.219792, step = 21588 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6104\n",
      "INFO:tensorflow:loss = 0.0, step = 21688 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5844\n",
      "INFO:tensorflow:loss = 0.47237054, step = 21788 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6194\n",
      "INFO:tensorflow:loss = 0.85011137, step = 21888 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.1619\n",
      "INFO:tensorflow:loss = 0.0, step = 21988 (2.692 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5734\n",
      "INFO:tensorflow:loss = 0.0, step = 22088 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4317\n",
      "INFO:tensorflow:loss = 0.70741177, step = 22188 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6547\n",
      "INFO:tensorflow:loss = 0.47171077, step = 22288 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6627\n",
      "INFO:tensorflow:loss = 0.5028372, step = 22388 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5908\n",
      "INFO:tensorflow:loss = 0.35681105, step = 22488 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5343\n",
      "INFO:tensorflow:loss = 0.2609094, step = 22588 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.0699\n",
      "INFO:tensorflow:loss = 0.6503695, step = 22688 (1.920 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5384\n",
      "INFO:tensorflow:loss = 0.5436743, step = 22788 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.355\n",
      "INFO:tensorflow:loss = 0.33172384, step = 22888 (1.986 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5318\n",
      "INFO:tensorflow:loss = 0.0, step = 22988 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.191\n",
      "INFO:tensorflow:loss = 1.5375553, step = 23088 (1.916 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3862\n",
      "INFO:tensorflow:loss = 0.79137015, step = 23188 (1.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6323\n",
      "INFO:tensorflow:loss = 1.066259, step = 23288 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5777\n",
      "INFO:tensorflow:loss = 0.24582863, step = 23388 (2.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.6897\n",
      "INFO:tensorflow:loss = 0.0, step = 23488 (2.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6339\n",
      "INFO:tensorflow:loss = 0.87757206, step = 23588 (2.350 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.3457\n",
      "INFO:tensorflow:loss = 0.55600107, step = 23688 (1.911 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5314\n",
      "INFO:tensorflow:loss = 0.47083998, step = 23788 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.451\n",
      "INFO:tensorflow:loss = 1.4212315, step = 23888 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5595\n",
      "INFO:tensorflow:loss = 0.0, step = 23988 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6322\n",
      "INFO:tensorflow:loss = 0.0, step = 24088 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4873\n",
      "INFO:tensorflow:loss = 0.630314, step = 24188 (1.988 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.2251\n",
      "INFO:tensorflow:loss = 0.53927445, step = 24288 (1.908 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 52.5387\n",
      "INFO:tensorflow:loss = 0.0, step = 24388 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6085\n",
      "INFO:tensorflow:loss = 0.5944677, step = 24488 (2.055 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.5018\n",
      "INFO:tensorflow:loss = 0.487154, step = 24588 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5256\n",
      "INFO:tensorflow:loss = 0.6628181, step = 24688 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5446\n",
      "INFO:tensorflow:loss = 0.19179669, step = 24788 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4385\n",
      "INFO:tensorflow:loss = 0.30279884, step = 24888 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4941\n",
      "INFO:tensorflow:loss = 0.59198093, step = 24988 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4964\n",
      "INFO:tensorflow:loss = 0.55742455, step = 25088 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5192\n",
      "INFO:tensorflow:loss = 0.82495356, step = 25188 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4819\n",
      "INFO:tensorflow:loss = 1.1238471, step = 25288 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.61\n",
      "INFO:tensorflow:loss = 0.947902, step = 25388 (2.654 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.5253\n",
      "INFO:tensorflow:loss = 0.0, step = 25488 (1.912 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6119\n",
      "INFO:tensorflow:loss = 0.9612102, step = 25588 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7735\n",
      "INFO:tensorflow:loss = 0.7448259, step = 25688 (1.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4373\n",
      "INFO:tensorflow:loss = 0.33027744, step = 25788 (1.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.7362\n",
      "INFO:tensorflow:loss = 1.1553638, step = 25888 (1.932 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3551\n",
      "INFO:tensorflow:loss = 0.6183547, step = 25988 (1.910 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5333\n",
      "INFO:tensorflow:loss = 0.5815048, step = 26088 (2.011 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.6992\n",
      "INFO:tensorflow:loss = 0.01952362, step = 26188 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4901\n",
      "INFO:tensorflow:loss = 0.21779919, step = 26288 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5261\n",
      "INFO:tensorflow:loss = 0.0, step = 26388 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.5906\n",
      "INFO:tensorflow:loss = 1.4852704, step = 26488 (2.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5412\n",
      "INFO:tensorflow:loss = 0.9015351, step = 26588 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.594\n",
      "INFO:tensorflow:loss = 0.0, step = 26688 (2.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.1306\n",
      "INFO:tensorflow:loss = 1.0390706, step = 26788 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6015\n",
      "INFO:tensorflow:loss = 1.056304, step = 26888 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5613\n",
      "INFO:tensorflow:loss = 0.09175587, step = 26988 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5859\n",
      "INFO:tensorflow:loss = 0.0, step = 27088 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5139\n",
      "INFO:tensorflow:loss = 0.83794874, step = 27188 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5751\n",
      "INFO:tensorflow:loss = 0.8135781, step = 27288 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4086\n",
      "INFO:tensorflow:loss = 0.1377449, step = 27388 (1.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.8924\n",
      "INFO:tensorflow:loss = 0.869509, step = 27488 (1.927 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.9526\n",
      "INFO:tensorflow:loss = 1.0113224, step = 27588 (2.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.0271\n",
      "INFO:tensorflow:loss = 0.16829681, step = 27688 (1.926 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2726\n",
      "INFO:tensorflow:loss = 0.0, step = 27788 (1.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.7091\n",
      "INFO:tensorflow:loss = 0.4035883, step = 27888 (1.933 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1656\n",
      "INFO:tensorflow:loss = 0.0, step = 27988 (1.917 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4549\n",
      "INFO:tensorflow:loss = 0.35576725, step = 28088 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.272\n",
      "INFO:tensorflow:loss = 0.0, step = 28188 (1.913 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1499\n",
      "INFO:tensorflow:loss = 1.2739187, step = 28288 (1.917 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2478\n",
      "INFO:tensorflow:loss = 0.0, step = 28388 (1.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3698\n",
      "INFO:tensorflow:loss = 1.23036, step = 28488 (1.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.8976\n",
      "INFO:tensorflow:loss = 0.4460435, step = 28588 (1.927 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.9139\n",
      "INFO:tensorflow:loss = 0.84350604, step = 28688 (1.927 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4012\n",
      "INFO:tensorflow:loss = 0.0, step = 28788 (1.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4271\n",
      "INFO:tensorflow:loss = 0.4203466, step = 28888 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5016\n",
      "INFO:tensorflow:loss = 0.33189702, step = 28988 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2363\n",
      "INFO:tensorflow:loss = 0.61088, step = 29088 (1.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.0118\n",
      "INFO:tensorflow:loss = 0.29475927, step = 29188 (1.923 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2607\n",
      "INFO:tensorflow:loss = 1.1887534, step = 29288 (1.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1136\n",
      "INFO:tensorflow:loss = 0.0, step = 29388 (1.919 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1474\n",
      "INFO:tensorflow:loss = 0.0, step = 29488 (2.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.4759\n",
      "INFO:tensorflow:loss = 0.0, step = 29588 (1.926 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.332\n",
      "INFO:tensorflow:loss = 0.11485386, step = 29688 (1.911 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1933\n",
      "INFO:tensorflow:loss = 0.44538435, step = 29788 (1.916 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.9201\n",
      "INFO:tensorflow:loss = 0.86519176, step = 29888 (1.925 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.0293\n",
      "INFO:tensorflow:loss = 0.0, step = 29988 (1.922 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5314\n",
      "INFO:tensorflow:loss = 0.5682808, step = 30088 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4904\n",
      "INFO:tensorflow:loss = 1.5787185, step = 30188 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2848\n",
      "INFO:tensorflow:loss = 0.3839233, step = 30288 (1.912 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3755\n",
      "INFO:tensorflow:loss = 0.0, step = 30388 (1.974 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.259\n",
      "INFO:tensorflow:loss = 0.5752179, step = 30488 (1.925 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3132\n",
      "INFO:tensorflow:loss = 0.9484272, step = 30588 (1.912 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.8658\n",
      "INFO:tensorflow:loss = 0.6658233, step = 30688 (1.928 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.0878\n",
      "INFO:tensorflow:loss = 0.6430267, step = 30788 (1.920 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.831\n",
      "INFO:tensorflow:loss = 1.0917994, step = 30888 (1.930 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.6767\n",
      "INFO:tensorflow:loss = 0.0, step = 30988 (1.934 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.0351\n",
      "INFO:tensorflow:loss = 0.0, step = 31088 (1.922 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3742\n",
      "INFO:tensorflow:loss = 0.6434305, step = 31188 (1.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2058\n",
      "INFO:tensorflow:loss = 1.0859432, step = 31288 (1.916 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3867\n",
      "INFO:tensorflow:loss = 0.0, step = 31388 (1.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5418\n",
      "INFO:tensorflow:loss = 0.0, step = 31488 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.0346\n",
      "INFO:tensorflow:loss = 0.0, step = 31588 (1.922 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3755\n",
      "INFO:tensorflow:loss = 1.5273474, step = 31688 (1.910 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2765\n",
      "INFO:tensorflow:loss = 0.0, step = 31788 (1.912 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.51\n",
      "INFO:tensorflow:loss = 1.0477495, step = 31888 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3917\n",
      "INFO:tensorflow:loss = 0.0, step = 31988 (1.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4943\n",
      "INFO:tensorflow:loss = 0.70088726, step = 32088 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4921\n",
      "INFO:tensorflow:loss = 1.1340637, step = 32188 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3422\n",
      "INFO:tensorflow:loss = 0.0, step = 32288 (1.911 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2254\n",
      "INFO:tensorflow:loss = 0.0, step = 32388 (1.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3648\n",
      "INFO:tensorflow:loss = 0.0, step = 32488 (1.911 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3881\n",
      "INFO:tensorflow:loss = 0.0, step = 32588 (1.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4442\n",
      "INFO:tensorflow:loss = 1.4689279, step = 32688 (1.907 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 52.4477\n",
      "INFO:tensorflow:loss = 0.0, step = 32788 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.9564\n",
      "INFO:tensorflow:loss = 0.0, step = 32888 (1.925 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4405\n",
      "INFO:tensorflow:loss = 0.9281349, step = 32988 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.45\n",
      "INFO:tensorflow:loss = 0.5142336, step = 33088 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4975\n",
      "INFO:tensorflow:loss = 0.55052865, step = 33188 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5643\n",
      "INFO:tensorflow:loss = 0.8490486, step = 33288 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.44\n",
      "INFO:tensorflow:loss = 0.8341398, step = 33388 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4571\n",
      "INFO:tensorflow:loss = 1.4561552, step = 33488 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4795\n",
      "INFO:tensorflow:loss = 0.26764202, step = 33588 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5178\n",
      "INFO:tensorflow:loss = 1.3634038, step = 33688 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.503\n",
      "INFO:tensorflow:loss = 0.0, step = 33788 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5094\n",
      "INFO:tensorflow:loss = 0.2753652, step = 33888 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5154\n",
      "INFO:tensorflow:loss = 1.0928192, step = 33988 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3848\n",
      "INFO:tensorflow:loss = 0.04875183, step = 34088 (1.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4751\n",
      "INFO:tensorflow:loss = 0.25798702, step = 34188 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.498\n",
      "INFO:tensorflow:loss = 0.0, step = 34288 (1.905 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 34376 into D:/Retinal_Datasets/models/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0.\n",
      "INFO:tensorflow:Evaluation on test set.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-12-14:26:41\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:/Retinal_Datasets/models/model.ckpt-34376\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-12-14:26:43\n",
      "INFO:tensorflow:Saving dict for global step 34376: embedding_mean_norm = 17.952206, fraction_positive_triplets = 0.002737634, global_step = 34376, loss = 1.5377716\n",
      "embedding_mean_norm: 17.952205657958984\n",
      "fraction_positive_triplets: 0.0027376338839530945\n",
      "loss: 1.5377715826034546\n",
      "global_step: 34376\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Load the parameters from json file\n",
    "#args = parser.parse_args()\n",
    "json_path = 'D:/Retinal_Datasets/params.json'\n",
    "assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "params = Params(json_path)\n",
    "\n",
    "# # Set up logging for predictions\n",
    "# tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "# logging_hook = tf.train.LoggingTensorHook(\n",
    "#     tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "# Define the model\n",
    "tf.logging.info(\"Creating the model...\")\n",
    "config = tf.estimator.RunConfig(tf_random_seed=230,\n",
    "                                model_dir='D:/Retinal_Datasets/models/',\n",
    "                                save_summary_steps=params.save_summary_steps)\n",
    "estimator = tf.estimator.Estimator(model_fn, params=params, config=config)\n",
    "\n",
    "# Train the model\n",
    "tf.logging.info(\"Starting training for {} epoch(s).\".format(params.num_epochs))\n",
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": train_data},\n",
    "    y=train_labels,\n",
    "    batch_size=params.batch_size,\n",
    "    num_epochs=params.num_epochs,\n",
    "    shuffle=True)\n",
    "estimator.train(input_fn=train_input_fn)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "tf.logging.info(\"Evaluation on test set.\")\n",
    "# Evaluate the model and print results\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": eval_data},\n",
    "    y=eval_labels,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "res = estimator.evaluate(input_fn=eval_input_fn)\n",
    "for key in res:\n",
    "    print(\"{}: {}\".format(key, res[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_mean_norm': 10.46597, 'fraction_positive_triplets': 0.003218118, 'loss': 0.91916716, 'global_step': 17188}\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
