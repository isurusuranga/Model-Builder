{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation, GlobalAveragePooling2D, GlobalMaxPooling2D, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from keras.utils import layer_utils, np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "#from keras.applications.resnet50 import preprocess_input\n",
    "from keras import Model\n",
    "from keras import initializers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import resnet\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from classification_models import ResNet18\n",
    "from classification_models.resnet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = 224\n",
    "image_height = 224\n",
    "\n",
    "# Change the batchsize according to your system RAM\n",
    "train_batchsize = 64\n",
    "val_batchsize = 64\n",
    "\n",
    "train_dir = \"D:/retinal_data_set_visioncare/TrainValidationData/augmented_train_data_224\"\n",
    "validation_dir = \"D:/retinal_data_set_visioncare/TrainValidationData/validation\"\n",
    "test_dir = \"D:/retinal_data_set_visioncare/newTrainValidationTestData/new_test\"\n",
    "\n",
    "#train_dir = \"D:/retinal_data_set_visioncare/New_Train_Test_Data/train\"\n",
    "#validation_dir = \"D:/retinal_data_set_visioncare/TrainValidationData/validation\"\n",
    "#test_dir = \"D:/retinal_data_set_visioncare/New_Train_Test_Data/test\"\n",
    "\n",
    "model_check_point_loc = 'D:/retinal_data_set_visioncare/models/resnet18/resnet_updated_all_cnn_dr.h5'\n",
    "\n",
    "#resnet_base = applications.ResNet50(weights='imagenet', include_top=False, input_shape=(image_width, image_height, 3))\n",
    "#resnet_base = resnet.ResNet101(weights='imagenet', include_top=False, input_shape=(image_width, image_height, 3))\n",
    "resnet_base = ResNet18(weights='imagenet', include_top=False, input_shape=(image_width, image_height, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.topology.InputLayer object at 0x000002AB86406EF0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000002AB86406A20> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x000002AB86406CC0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB8642E668> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000002AB86406E80> False\n",
      "<keras.layers.core.Activation object at 0x000002AB851DC7F0> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x000002AB851EEDD8> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000002AB851DC2B0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000002AB8527BDD8> False\n",
      "<keras.layers.core.Activation object at 0x000002AB852D0940> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x000002AB852D7B38> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB852D0BA8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000002AB85319208> False\n",
      "<keras.layers.core.Activation object at 0x000002AB853A9EB8> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x000002AB853CE5C0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB853BAD30> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB854106A0> False\n",
      "<keras.layers.merge.Add object at 0x000002AB854B0320> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000002AB854B0550> False\n",
      "<keras.layers.core.Activation object at 0x000002AB854C3940> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x000002AB854C3278> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB854DB828> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000002AB8552D0B8> False\n",
      "<keras.layers.core.Activation object at 0x000002AB855BBDD8> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x000002AB855BBEF0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB855CD9E8> False\n",
      "<keras.layers.merge.Add object at 0x000002AB85624588> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000002AB856C1208> False\n",
      "<keras.layers.core.Activation object at 0x000002AB856C1198> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x000002AB856C1320> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB856D45F8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000002AB8570BCF8> False\n",
      "<keras.layers.core.Activation object at 0x000002AB857B9F98> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x000002AB857DFBA8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB857B9978> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB8581D278> False\n",
      "<keras.layers.merge.Add object at 0x000002AB858ACEF0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000002AB858BE128> False\n",
      "<keras.layers.core.Activation object at 0x000002AB858BE0F0> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x000002AB858E83C8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB858D0358> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000002AB85924BE0> False\n",
      "<keras.layers.core.Activation object at 0x000002AB859D4FD0> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x000002AB859F4A90> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB859D4860> False\n",
      "<keras.layers.merge.Add object at 0x000002AB85A34160> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000002AB85AC1DD8> False\n",
      "<keras.layers.core.Activation object at 0x000002AB85AC1E80> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x000002AB85AEB7F0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB85AD3CC0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000002AB85B28898> False\n",
      "<keras.layers.core.Activation object at 0x000002AB85BCDA20> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x000002AB85BF1748> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB85BCD518> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB85C15DA0> False\n",
      "<keras.layers.merge.Add object at 0x000002AB85CC2A20> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000002AB85CC2C50> False\n",
      "<keras.layers.core.Activation object at 0x000002AB85CD2C18> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x000002AB85D190B8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB85CEA860> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000002AB85D3B780> False\n",
      "<keras.layers.core.Activation object at 0x000002AB85DDE908> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x000002AB85E07630> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB85DDE400> False\n",
      "<keras.layers.merge.Add object at 0x000002AB85E2FC88> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000002AB85ED9978> False\n",
      "<keras.layers.core.Activation object at 0x000002AB85ED9908> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x000002AB85ED9A20> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB85EE4CF8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000002AB85FA2470> False\n",
      "<keras.layers.core.Activation object at 0x000002AB85FDC828> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x000002AB86006240> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB85FDC128> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB86031940> False\n",
      "<keras.layers.merge.Add object at 0x000002AB864435C0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000002AB864437F0> False\n",
      "<keras.layers.core.Activation object at 0x000002AB86456E48> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x000002AB864561D0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB8646DAC8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000002AB864B9320> False\n",
      "<keras.layers.core.Activation object at 0x000002AB8654DCC0> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x000002AB86572748> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002AB8655FB70> False\n",
      "<keras.layers.merge.Add object at 0x000002AB865B3828> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000002AB85228BE0> False\n",
      "<keras.layers.core.Activation object at 0x000002AB8666C5F8> False\n"
     ]
    }
   ],
   "source": [
    "# Freeze the layers except the last 2 layers\n",
    "for layer in resnet_base.layers[:]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Check the trainable status of the individual layers\n",
    "for layer in resnet_base.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "data (InputLayer)               (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bn_data (BatchNormalization)    (None, 224, 224, 3)  9           data[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_55 (ZeroPadding2 (None, 230, 230, 3)  0           bn_data[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv0 (Conv2D)                  (None, 112, 112, 64) 9408        zero_padding2d_55[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn0 (BatchNormalization)        (None, 112, 112, 64) 256         conv0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "relu0 (Activation)              (None, 112, 112, 64) 0           bn0[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_56 (ZeroPadding2 (None, 114, 114, 64) 0           relu0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pooling0 (MaxPooling2D)         (None, 56, 56, 64)   0           zero_padding2d_56[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_bn1 (BatchNormaliz (None, 56, 56, 64)   256         pooling0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_relu1 (Activation) (None, 56, 56, 64)   0           stage1_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_57 (ZeroPadding2 (None, 58, 58, 64)   0           stage1_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_conv1 (Conv2D)     (None, 56, 56, 64)   36864       zero_padding2d_57[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_bn2 (BatchNormaliz (None, 56, 56, 64)   256         stage1_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_relu2 (Activation) (None, 56, 56, 64)   0           stage1_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_58 (ZeroPadding2 (None, 58, 58, 64)   0           stage1_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_conv2 (Conv2D)     (None, 56, 56, 64)   36864       zero_padding2d_58[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_sc (Conv2D)        (None, 56, 56, 64)   4096        stage1_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 56, 56, 64)   0           stage1_unit1_conv2[0][0]         \n",
      "                                                                 stage1_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_bn1 (BatchNormaliz (None, 56, 56, 64)   256         add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_relu1 (Activation) (None, 56, 56, 64)   0           stage1_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_59 (ZeroPadding2 (None, 58, 58, 64)   0           stage1_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_conv1 (Conv2D)     (None, 56, 56, 64)   36864       zero_padding2d_59[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_bn2 (BatchNormaliz (None, 56, 56, 64)   256         stage1_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_relu2 (Activation) (None, 56, 56, 64)   0           stage1_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_60 (ZeroPadding2 (None, 58, 58, 64)   0           stage1_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_conv2 (Conv2D)     (None, 56, 56, 64)   36864       zero_padding2d_60[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 56, 56, 64)   0           stage1_unit2_conv2[0][0]         \n",
      "                                                                 add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_bn1 (BatchNormaliz (None, 56, 56, 64)   256         add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_relu1 (Activation) (None, 56, 56, 64)   0           stage2_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_61 (ZeroPadding2 (None, 58, 58, 64)   0           stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_conv1 (Conv2D)     (None, 28, 28, 128)  73728       zero_padding2d_61[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_bn2 (BatchNormaliz (None, 28, 28, 128)  512         stage2_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_relu2 (Activation) (None, 28, 28, 128)  0           stage2_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_62 (ZeroPadding2 (None, 30, 30, 128)  0           stage2_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_conv2 (Conv2D)     (None, 28, 28, 128)  147456      zero_padding2d_62[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_sc (Conv2D)        (None, 28, 28, 128)  8192        stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 28, 28, 128)  0           stage2_unit1_conv2[0][0]         \n",
      "                                                                 stage2_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_bn1 (BatchNormaliz (None, 28, 28, 128)  512         add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_relu1 (Activation) (None, 28, 28, 128)  0           stage2_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_63 (ZeroPadding2 (None, 30, 30, 128)  0           stage2_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_conv1 (Conv2D)     (None, 28, 28, 128)  147456      zero_padding2d_63[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_bn2 (BatchNormaliz (None, 28, 28, 128)  512         stage2_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_relu2 (Activation) (None, 28, 28, 128)  0           stage2_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_64 (ZeroPadding2 (None, 30, 30, 128)  0           stage2_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_conv2 (Conv2D)     (None, 28, 28, 128)  147456      zero_padding2d_64[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 28, 28, 128)  0           stage2_unit2_conv2[0][0]         \n",
      "                                                                 add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_bn1 (BatchNormaliz (None, 28, 28, 128)  512         add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_relu1 (Activation) (None, 28, 28, 128)  0           stage3_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_65 (ZeroPadding2 (None, 30, 30, 128)  0           stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_conv1 (Conv2D)     (None, 14, 14, 256)  294912      zero_padding2d_65[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_bn2 (BatchNormaliz (None, 14, 14, 256)  1024        stage3_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_relu2 (Activation) (None, 14, 14, 256)  0           stage3_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_66 (ZeroPadding2 (None, 16, 16, 256)  0           stage3_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_conv2 (Conv2D)     (None, 14, 14, 256)  589824      zero_padding2d_66[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_sc (Conv2D)        (None, 14, 14, 256)  32768       stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 14, 14, 256)  0           stage3_unit1_conv2[0][0]         \n",
      "                                                                 stage3_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_bn1 (BatchNormaliz (None, 14, 14, 256)  1024        add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_relu1 (Activation) (None, 14, 14, 256)  0           stage3_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_67 (ZeroPadding2 (None, 16, 16, 256)  0           stage3_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_conv1 (Conv2D)     (None, 14, 14, 256)  589824      zero_padding2d_67[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_bn2 (BatchNormaliz (None, 14, 14, 256)  1024        stage3_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_relu2 (Activation) (None, 14, 14, 256)  0           stage3_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_68 (ZeroPadding2 (None, 16, 16, 256)  0           stage3_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_conv2 (Conv2D)     (None, 14, 14, 256)  589824      zero_padding2d_68[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 14, 14, 256)  0           stage3_unit2_conv2[0][0]         \n",
      "                                                                 add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_bn1 (BatchNormaliz (None, 14, 14, 256)  1024        add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_relu1 (Activation) (None, 14, 14, 256)  0           stage4_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_69 (ZeroPadding2 (None, 16, 16, 256)  0           stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_conv1 (Conv2D)     (None, 7, 7, 512)    1179648     zero_padding2d_69[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_bn2 (BatchNormaliz (None, 7, 7, 512)    2048        stage4_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_relu2 (Activation) (None, 7, 7, 512)    0           stage4_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_70 (ZeroPadding2 (None, 9, 9, 512)    0           stage4_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_conv2 (Conv2D)     (None, 7, 7, 512)    2359296     zero_padding2d_70[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_sc (Conv2D)        (None, 7, 7, 512)    131072      stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 7, 7, 512)    0           stage4_unit1_conv2[0][0]         \n",
      "                                                                 stage4_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_bn1 (BatchNormaliz (None, 7, 7, 512)    2048        add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_relu1 (Activation) (None, 7, 7, 512)    0           stage4_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_71 (ZeroPadding2 (None, 9, 9, 512)    0           stage4_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_conv1 (Conv2D)     (None, 7, 7, 512)    2359296     zero_padding2d_71[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_bn2 (BatchNormaliz (None, 7, 7, 512)    2048        stage4_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_relu2 (Activation) (None, 7, 7, 512)    0           stage4_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_72 (ZeroPadding2 (None, 9, 9, 512)    0           stage4_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_conv2 (Conv2D)     (None, 7, 7, 512)    2359296     zero_padding2d_72[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 7, 7, 512)    0           stage4_unit2_conv2[0][0]         \n",
      "                                                                 add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bn1 (BatchNormalization)        (None, 7, 7, 512)    2048        add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 7, 7, 512)    0           bn1[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 11,186,889\n",
      "Trainable params: 0\n",
      "Non-trainable params: 11,186,889\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 images belonging to 5 classes.\n",
      "Found 278 images belonging to 5 classes.\n",
      "Found 142 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(image_width, image_height),\n",
    "        batch_size=train_batchsize,\n",
    "        shuffle=True,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(image_width, image_height),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(image_width, image_height),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "data (InputLayer)               (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bn_data (BatchNormalization)    (None, 224, 224, 3)  9           data[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_55 (ZeroPadding2 (None, 230, 230, 3)  0           bn_data[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv0 (Conv2D)                  (None, 112, 112, 64) 9408        zero_padding2d_55[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn0 (BatchNormalization)        (None, 112, 112, 64) 256         conv0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "relu0 (Activation)              (None, 112, 112, 64) 0           bn0[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_56 (ZeroPadding2 (None, 114, 114, 64) 0           relu0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pooling0 (MaxPooling2D)         (None, 56, 56, 64)   0           zero_padding2d_56[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_bn1 (BatchNormaliz (None, 56, 56, 64)   256         pooling0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_relu1 (Activation) (None, 56, 56, 64)   0           stage1_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_57 (ZeroPadding2 (None, 58, 58, 64)   0           stage1_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_conv1 (Conv2D)     (None, 56, 56, 64)   36864       zero_padding2d_57[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_bn2 (BatchNormaliz (None, 56, 56, 64)   256         stage1_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_relu2 (Activation) (None, 56, 56, 64)   0           stage1_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_58 (ZeroPadding2 (None, 58, 58, 64)   0           stage1_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_conv2 (Conv2D)     (None, 56, 56, 64)   36864       zero_padding2d_58[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_sc (Conv2D)        (None, 56, 56, 64)   4096        stage1_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 56, 56, 64)   0           stage1_unit1_conv2[0][0]         \n",
      "                                                                 stage1_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_bn1 (BatchNormaliz (None, 56, 56, 64)   256         add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_relu1 (Activation) (None, 56, 56, 64)   0           stage1_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_59 (ZeroPadding2 (None, 58, 58, 64)   0           stage1_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_conv1 (Conv2D)     (None, 56, 56, 64)   36864       zero_padding2d_59[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_bn2 (BatchNormaliz (None, 56, 56, 64)   256         stage1_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_relu2 (Activation) (None, 56, 56, 64)   0           stage1_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_60 (ZeroPadding2 (None, 58, 58, 64)   0           stage1_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_conv2 (Conv2D)     (None, 56, 56, 64)   36864       zero_padding2d_60[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 56, 56, 64)   0           stage1_unit2_conv2[0][0]         \n",
      "                                                                 add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_bn1 (BatchNormaliz (None, 56, 56, 64)   256         add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_relu1 (Activation) (None, 56, 56, 64)   0           stage2_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_61 (ZeroPadding2 (None, 58, 58, 64)   0           stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_conv1 (Conv2D)     (None, 28, 28, 128)  73728       zero_padding2d_61[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_bn2 (BatchNormaliz (None, 28, 28, 128)  512         stage2_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_relu2 (Activation) (None, 28, 28, 128)  0           stage2_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_62 (ZeroPadding2 (None, 30, 30, 128)  0           stage2_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_conv2 (Conv2D)     (None, 28, 28, 128)  147456      zero_padding2d_62[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_sc (Conv2D)        (None, 28, 28, 128)  8192        stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 28, 28, 128)  0           stage2_unit1_conv2[0][0]         \n",
      "                                                                 stage2_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_bn1 (BatchNormaliz (None, 28, 28, 128)  512         add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_relu1 (Activation) (None, 28, 28, 128)  0           stage2_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_63 (ZeroPadding2 (None, 30, 30, 128)  0           stage2_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_conv1 (Conv2D)     (None, 28, 28, 128)  147456      zero_padding2d_63[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_bn2 (BatchNormaliz (None, 28, 28, 128)  512         stage2_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_relu2 (Activation) (None, 28, 28, 128)  0           stage2_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_64 (ZeroPadding2 (None, 30, 30, 128)  0           stage2_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_conv2 (Conv2D)     (None, 28, 28, 128)  147456      zero_padding2d_64[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 28, 28, 128)  0           stage2_unit2_conv2[0][0]         \n",
      "                                                                 add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_bn1 (BatchNormaliz (None, 28, 28, 128)  512         add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_relu1 (Activation) (None, 28, 28, 128)  0           stage3_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_65 (ZeroPadding2 (None, 30, 30, 128)  0           stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_conv1 (Conv2D)     (None, 14, 14, 256)  294912      zero_padding2d_65[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_bn2 (BatchNormaliz (None, 14, 14, 256)  1024        stage3_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_relu2 (Activation) (None, 14, 14, 256)  0           stage3_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_66 (ZeroPadding2 (None, 16, 16, 256)  0           stage3_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_conv2 (Conv2D)     (None, 14, 14, 256)  589824      zero_padding2d_66[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_sc (Conv2D)        (None, 14, 14, 256)  32768       stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 14, 14, 256)  0           stage3_unit1_conv2[0][0]         \n",
      "                                                                 stage3_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_bn1 (BatchNormaliz (None, 14, 14, 256)  1024        add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_relu1 (Activation) (None, 14, 14, 256)  0           stage3_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_67 (ZeroPadding2 (None, 16, 16, 256)  0           stage3_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_conv1 (Conv2D)     (None, 14, 14, 256)  589824      zero_padding2d_67[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_bn2 (BatchNormaliz (None, 14, 14, 256)  1024        stage3_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_relu2 (Activation) (None, 14, 14, 256)  0           stage3_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_68 (ZeroPadding2 (None, 16, 16, 256)  0           stage3_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_conv2 (Conv2D)     (None, 14, 14, 256)  589824      zero_padding2d_68[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 14, 14, 256)  0           stage3_unit2_conv2[0][0]         \n",
      "                                                                 add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_bn1 (BatchNormaliz (None, 14, 14, 256)  1024        add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_relu1 (Activation) (None, 14, 14, 256)  0           stage4_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_69 (ZeroPadding2 (None, 16, 16, 256)  0           stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_conv1 (Conv2D)     (None, 7, 7, 512)    1179648     zero_padding2d_69[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_bn2 (BatchNormaliz (None, 7, 7, 512)    2048        stage4_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_relu2 (Activation) (None, 7, 7, 512)    0           stage4_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_70 (ZeroPadding2 (None, 9, 9, 512)    0           stage4_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_conv2 (Conv2D)     (None, 7, 7, 512)    2359296     zero_padding2d_70[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_sc (Conv2D)        (None, 7, 7, 512)    131072      stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 7, 7, 512)    0           stage4_unit1_conv2[0][0]         \n",
      "                                                                 stage4_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_bn1 (BatchNormaliz (None, 7, 7, 512)    2048        add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_relu1 (Activation) (None, 7, 7, 512)    0           stage4_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_71 (ZeroPadding2 (None, 9, 9, 512)    0           stage4_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_conv1 (Conv2D)     (None, 7, 7, 512)    2359296     zero_padding2d_71[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_bn2 (BatchNormaliz (None, 7, 7, 512)    2048        stage4_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_relu2 (Activation) (None, 7, 7, 512)    0           stage4_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_72 (ZeroPadding2 (None, 9, 9, 512)    0           stage4_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_conv2 (Conv2D)     (None, 7, 7, 512)    2359296     zero_padding2d_72[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 7, 7, 512)    0           stage4_unit2_conv2[0][0]         \n",
      "                                                                 add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bn1 (BatchNormalization)        (None, 7, 7, 512)    2048        add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 7, 7, 512)    0           bn1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 512)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_avg_dropout (Dropout)    (None, 512)          0           global_average_pooling2d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 5)            2560        global_avg_dropout[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 5)            20          dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 5)            0           batch_normalization_6[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 11,189,469\n",
      "Trainable params: 2,570\n",
      "Non-trainable params: 11,186,899\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the model\n",
    "# add a global spatial average pooling layer 28 , 28 , 128\n",
    "x = resnet_base.get_layer(index=-1).output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.7, name='global_avg_dropout')(x)\n",
    "# add a fully-connected layer\n",
    "# x = Dense(128, kernel_initializer=initializers.he_normal(seed=None), kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01), use_bias=False)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation('relu')(x)\n",
    "# x = Dropout(0.5, name='fc1_dropout')(x)\n",
    "# x = Dense(7, kernel_initializer=initializers.he_normal(seed=None), use_bias=False, kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation('relu')(x)\n",
    "# x = Dropout(0.5, name='fc2_dropout')(x)\n",
    "# and a fully connected output/classification layer\n",
    "#x = Dense(5, kernel_initializer=initializers.he_normal(seed=None))(x)\n",
    "#x = Dense(5, activity_regularizer=regularizers.l2(0.001), use_bias=False)(x)\n",
    "#kernel_regularizer=regularizers.l2(0.001), activity_regularizer=regularizers.l2(0.001)\n",
    "x = Dense(5, kernel_initializer=initializers.he_normal(seed=None), activity_regularizer=regularizers.l2(0.001), use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "predictions = Activation('softmax')(x)\n",
    "# create the full network so we can train on it\n",
    "model = Model(inputs=resnet_base.input, outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Conv2D(4, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "# #model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(GlobalAveragePooling2D())\n",
    "# #model.add(Flatten())\n",
    "# model.add(Dense(15, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# model.summary()\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=optimizers.Adadelta(),\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(lr=1e-3),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(model_check_point_loc, monitor='val_acc', verbose=0, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights_dic = {0: 0.37236180904522614,\n",
    "#  1: 1.5278350515463917,\n",
    "#  2: 1.2453781512605042,\n",
    "#  3: 2.9058823529411764,\n",
    "#  4: 1.95}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights_dic\n",
    "# class_weight=class_weights_dic,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "78/78 [==============================] - 74s 943ms/step - loss: 2.6896 - acc: 0.2560 - val_loss: 1.6036 - val_acc: 0.1942\n",
      "Epoch 2/100\n",
      "78/78 [==============================] - 68s 869ms/step - loss: 2.0056 - acc: 0.3678 - val_loss: 1.4611 - val_acc: 0.3204\n",
      "Epoch 3/100\n",
      "78/78 [==============================] - 68s 874ms/step - loss: 1.6030 - acc: 0.4499 - val_loss: 1.3976 - val_acc: 0.5971\n",
      "Epoch 4/100\n",
      "78/78 [==============================] - 68s 876ms/step - loss: 1.3740 - acc: 0.5144 - val_loss: 1.2677 - val_acc: 0.7476\n",
      "Epoch 5/100\n",
      "78/78 [==============================] - 69s 884ms/step - loss: 1.2826 - acc: 0.5323 - val_loss: 1.1385 - val_acc: 0.7379\n",
      "Epoch 6/100\n",
      "78/78 [==============================] - 69s 886ms/step - loss: 1.2785 - acc: 0.5101 - val_loss: 1.1359 - val_acc: 0.7330\n",
      "Epoch 7/100\n",
      "78/78 [==============================] - 69s 888ms/step - loss: 1.2700 - acc: 0.5164 - val_loss: 1.2150 - val_acc: 0.6845\n",
      "Epoch 8/100\n",
      "78/78 [==============================] - 68s 875ms/step - loss: 1.2507 - acc: 0.5258 - val_loss: 1.1596 - val_acc: 0.7379\n",
      "Epoch 9/100\n",
      "78/78 [==============================] - 68s 871ms/step - loss: 1.2595 - acc: 0.5200 - val_loss: 1.0887 - val_acc: 0.7621\n",
      "Epoch 10/100\n",
      "78/78 [==============================] - 68s 875ms/step - loss: 1.2400 - acc: 0.5203 - val_loss: 1.1434 - val_acc: 0.7087\n",
      "Epoch 11/100\n",
      "78/78 [==============================] - 69s 887ms/step - loss: 1.2176 - acc: 0.5375 - val_loss: 1.1818 - val_acc: 0.7087\n",
      "Epoch 12/100\n",
      "78/78 [==============================] - 69s 883ms/step - loss: 1.2299 - acc: 0.5271 - val_loss: 1.2431 - val_acc: 0.4272\n",
      "Epoch 13/100\n",
      "78/78 [==============================] - 69s 883ms/step - loss: 1.2248 - acc: 0.5361 - val_loss: 1.0426 - val_acc: 0.7282\n",
      "Epoch 14/100\n",
      "78/78 [==============================] - 68s 873ms/step - loss: 1.2398 - acc: 0.5247 - val_loss: 0.9919 - val_acc: 0.7379\n",
      "Epoch 15/100\n",
      "78/78 [==============================] - 68s 877ms/step - loss: 1.2448 - acc: 0.5341 - val_loss: 0.9997 - val_acc: 0.7621\n",
      "Epoch 16/100\n",
      "78/78 [==============================] - 69s 886ms/step - loss: 1.2311 - acc: 0.5347 - val_loss: 1.0206 - val_acc: 0.7379\n",
      "Epoch 17/100\n",
      "78/78 [==============================] - 68s 873ms/step - loss: 1.2120 - acc: 0.5339 - val_loss: 1.0112 - val_acc: 0.7573\n",
      "Epoch 18/100\n",
      "78/78 [==============================] - 69s 879ms/step - loss: 1.2125 - acc: 0.5378 - val_loss: 1.0052 - val_acc: 0.7670\n",
      "Epoch 19/100\n",
      "78/78 [==============================] - 69s 884ms/step - loss: 1.2259 - acc: 0.5305 - val_loss: 1.1548 - val_acc: 0.6845\n",
      "Epoch 20/100\n",
      "78/78 [==============================] - 69s 882ms/step - loss: 1.2066 - acc: 0.5371 - val_loss: 0.9698 - val_acc: 0.7330\n",
      "Epoch 21/100\n",
      "78/78 [==============================] - 69s 891ms/step - loss: 1.2186 - acc: 0.5339 - val_loss: 1.0509 - val_acc: 0.7524\n",
      "Epoch 22/100\n",
      "78/78 [==============================] - 68s 869ms/step - loss: 1.1973 - acc: 0.5319 - val_loss: 0.9759 - val_acc: 0.7233\n",
      "Epoch 23/100\n",
      "78/78 [==============================] - 69s 884ms/step - loss: 1.2242 - acc: 0.5288 - val_loss: 0.9907 - val_acc: 0.7233\n",
      "Epoch 24/100\n",
      "78/78 [==============================] - 68s 877ms/step - loss: 1.1853 - acc: 0.5418 - val_loss: 1.1133 - val_acc: 0.7233\n",
      "Epoch 25/100\n",
      "78/78 [==============================] - 70s 897ms/step - loss: 1.1931 - acc: 0.5503 - val_loss: 1.0337 - val_acc: 0.7621\n",
      "Epoch 26/100\n",
      "78/78 [==============================] - 69s 889ms/step - loss: 1.2192 - acc: 0.5190 - val_loss: 1.0729 - val_acc: 0.7282\n",
      "Epoch 27/100\n",
      "78/78 [==============================] - 69s 887ms/step - loss: 1.1833 - acc: 0.5505 - val_loss: 1.0806 - val_acc: 0.7379\n",
      "Epoch 28/100\n",
      "78/78 [==============================] - 67s 864ms/step - loss: 1.2148 - acc: 0.5254 - val_loss: 1.0566 - val_acc: 0.7524\n",
      "Epoch 29/100\n",
      "78/78 [==============================] - 68s 871ms/step - loss: 1.2076 - acc: 0.5393 - val_loss: 1.0648 - val_acc: 0.7718\n",
      "Epoch 30/100\n",
      "78/78 [==============================] - 68s 871ms/step - loss: 1.1927 - acc: 0.5374 - val_loss: 0.9377 - val_acc: 0.7184\n",
      "Epoch 31/100\n",
      "78/78 [==============================] - 69s 880ms/step - loss: 1.2093 - acc: 0.5339 - val_loss: 1.1221 - val_acc: 0.7330\n",
      "Epoch 32/100\n",
      "78/78 [==============================] - 69s 881ms/step - loss: 1.2188 - acc: 0.5327 - val_loss: 1.0909 - val_acc: 0.7330\n",
      "Epoch 33/100\n",
      "78/78 [==============================] - 68s 874ms/step - loss: 1.2031 - acc: 0.5322 - val_loss: 1.0537 - val_acc: 0.7330\n",
      "Epoch 34/100\n",
      "78/78 [==============================] - 68s 877ms/step - loss: 1.2244 - acc: 0.5341 - val_loss: 0.9355 - val_acc: 0.7184\n",
      "Epoch 35/100\n",
      "78/78 [==============================] - 69s 879ms/step - loss: 1.2060 - acc: 0.5387 - val_loss: 1.0924 - val_acc: 0.7718\n",
      "Epoch 36/100\n",
      "78/78 [==============================] - 67s 862ms/step - loss: 1.2076 - acc: 0.5389 - val_loss: 1.0718 - val_acc: 0.7767\n",
      "Epoch 37/100\n",
      "78/78 [==============================] - 69s 879ms/step - loss: 1.1976 - acc: 0.5421 - val_loss: 1.0549 - val_acc: 0.7184\n",
      "Epoch 38/100\n",
      "78/78 [==============================] - 68s 870ms/step - loss: 1.2164 - acc: 0.5263 - val_loss: 1.0986 - val_acc: 0.7184\n",
      "Epoch 39/100\n",
      "78/78 [==============================] - 69s 879ms/step - loss: 1.1873 - acc: 0.5389 - val_loss: 1.1450 - val_acc: 0.6165\n",
      "Epoch 40/100\n",
      "78/78 [==============================] - 69s 879ms/step - loss: 1.2012 - acc: 0.5346 - val_loss: 1.0351 - val_acc: 0.7573\n",
      "Epoch 41/100\n",
      "78/78 [==============================] - 68s 878ms/step - loss: 1.2152 - acc: 0.5297 - val_loss: 1.0985 - val_acc: 0.7282\n",
      "Epoch 42/100\n",
      "78/78 [==============================] - 68s 870ms/step - loss: 1.2095 - acc: 0.5307 - val_loss: 1.2337 - val_acc: 0.4660\n",
      "Epoch 43/100\n",
      "78/78 [==============================] - 68s 870ms/step - loss: 1.2610 - acc: 0.5187 - val_loss: 1.2003 - val_acc: 0.5291\n",
      "Epoch 44/100\n",
      "78/78 [==============================] - 68s 870ms/step - loss: 1.2057 - acc: 0.5545 - val_loss: 1.0613 - val_acc: 0.7573\n",
      "Epoch 45/100\n",
      "78/78 [==============================] - 67s 862ms/step - loss: 1.2115 - acc: 0.5311 - val_loss: 1.2327 - val_acc: 0.4709\n",
      "Epoch 46/100\n",
      "78/78 [==============================] - 69s 880ms/step - loss: 1.2015 - acc: 0.5335 - val_loss: 1.1416 - val_acc: 0.6699\n",
      "Epoch 47/100\n",
      "78/78 [==============================] - 69s 881ms/step - loss: 1.2145 - acc: 0.5317 - val_loss: 1.0994 - val_acc: 0.7524\n",
      "Epoch 48/100\n",
      "78/78 [==============================] - 69s 880ms/step - loss: 1.2121 - acc: 0.5379 - val_loss: 0.9587 - val_acc: 0.7282\n",
      "Epoch 49/100\n",
      "78/78 [==============================] - 67s 863ms/step - loss: 1.1877 - acc: 0.5466 - val_loss: 0.9471 - val_acc: 0.7184\n",
      "Epoch 50/100\n",
      "78/78 [==============================] - 68s 870ms/step - loss: 1.1946 - acc: 0.5376 - val_loss: 1.0395 - val_acc: 0.7282\n",
      "Epoch 51/100\n",
      "78/78 [==============================] - 68s 870ms/step - loss: 1.2185 - acc: 0.5339 - val_loss: 1.0504 - val_acc: 0.7573\n",
      "Epoch 52/100\n",
      "78/78 [==============================] - 69s 879ms/step - loss: 1.2179 - acc: 0.5264 - val_loss: 1.0659 - val_acc: 0.7233\n",
      "Epoch 53/100\n",
      "78/78 [==============================] - 67s 863ms/step - loss: 1.1953 - acc: 0.5394 - val_loss: 1.0903 - val_acc: 0.6456\n",
      "Epoch 54/100\n",
      "78/78 [==============================] - 69s 888ms/step - loss: 1.2149 - acc: 0.5246 - val_loss: 0.9990 - val_acc: 0.7670\n",
      "Epoch 55/100\n",
      "78/78 [==============================] - 69s 881ms/step - loss: 1.1851 - acc: 0.5481 - val_loss: 0.9862 - val_acc: 0.7282\n",
      "Epoch 56/100\n",
      "78/78 [==============================] - 68s 876ms/step - loss: 1.2015 - acc: 0.5360 - val_loss: 0.9672 - val_acc: 0.7136\n",
      "Epoch 57/100\n",
      "78/78 [==============================] - 67s 861ms/step - loss: 1.1991 - acc: 0.5421 - val_loss: 0.9580 - val_acc: 0.7427\n",
      "Epoch 58/100\n",
      "78/78 [==============================] - 69s 879ms/step - loss: 1.2139 - acc: 0.5262 - val_loss: 1.0473 - val_acc: 0.7816\n",
      "Epoch 59/100\n",
      "78/78 [==============================] - 68s 870ms/step - loss: 1.2180 - acc: 0.5205 - val_loss: 0.9461 - val_acc: 0.7184\n",
      "Epoch 60/100\n",
      "78/78 [==============================] - 68s 870ms/step - loss: 1.2037 - acc: 0.5389 - val_loss: 1.0084 - val_acc: 0.7427\n",
      "Epoch 61/100\n",
      "78/78 [==============================] - 68s 878ms/step - loss: 1.2025 - acc: 0.5222 - val_loss: 1.0379 - val_acc: 0.7621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "78/78 [==============================] - 69s 883ms/step - loss: 1.1771 - acc: 0.5517 - val_loss: 0.9869 - val_acc: 0.7670\n",
      "Epoch 63/100\n",
      "78/78 [==============================] - 68s 878ms/step - loss: 1.2015 - acc: 0.5348 - val_loss: 0.9801 - val_acc: 0.7767\n",
      "Epoch 64/100\n",
      "78/78 [==============================] - 68s 871ms/step - loss: 1.1842 - acc: 0.5491 - val_loss: 0.9671 - val_acc: 0.7233\n",
      "Epoch 65/100\n",
      "78/78 [==============================] - 67s 861ms/step - loss: 1.1795 - acc: 0.5417 - val_loss: 1.1178 - val_acc: 0.6117\n",
      "Epoch 66/100\n",
      "78/78 [==============================] - 69s 879ms/step - loss: 1.1993 - acc: 0.5296 - val_loss: 1.1592 - val_acc: 0.5631\n",
      "Epoch 67/100\n",
      "78/78 [==============================] - 68s 870ms/step - loss: 1.1983 - acc: 0.5345 - val_loss: 0.9334 - val_acc: 0.7379\n",
      "Epoch 68/100\n",
      "78/78 [==============================] - 68s 872ms/step - loss: 1.1937 - acc: 0.5368 - val_loss: 1.0581 - val_acc: 0.7864\n",
      "Epoch 69/100\n",
      "78/78 [==============================] - 67s 860ms/step - loss: 1.1892 - acc: 0.5531 - val_loss: 0.9840 - val_acc: 0.7767\n",
      "Epoch 70/100\n",
      "78/78 [==============================] - 68s 878ms/step - loss: 1.2068 - acc: 0.5443 - val_loss: 0.9780 - val_acc: 0.7233\n",
      "Epoch 71/100\n",
      "78/78 [==============================] - 68s 869ms/step - loss: 1.2088 - acc: 0.5317 - val_loss: 0.9919 - val_acc: 0.7524\n",
      "Epoch 72/100\n",
      "78/78 [==============================] - 68s 869ms/step - loss: 1.1999 - acc: 0.5339 - val_loss: 1.0023 - val_acc: 0.7718\n",
      "Epoch 73/100\n",
      "78/78 [==============================] - 68s 869ms/step - loss: 1.2058 - acc: 0.5327 - val_loss: 1.0400 - val_acc: 0.7718\n",
      "Epoch 74/100\n",
      "78/78 [==============================] - 68s 869ms/step - loss: 1.1990 - acc: 0.5345 - val_loss: 1.0149 - val_acc: 0.7087\n",
      "Epoch 75/100\n",
      "78/78 [==============================] - 68s 869ms/step - loss: 1.1905 - acc: 0.5372 - val_loss: 1.0249 - val_acc: 0.7524\n",
      "Epoch 76/100\n",
      "78/78 [==============================] - 68s 872ms/step - loss: 1.2208 - acc: 0.5235 - val_loss: 1.0065 - val_acc: 0.7816\n",
      "Epoch 77/100\n",
      "78/78 [==============================] - 68s 870ms/step - loss: 1.1730 - acc: 0.5523 - val_loss: 0.9987 - val_acc: 0.7767\n",
      "Epoch 78/100\n",
      "78/78 [==============================] - 68s 871ms/step - loss: 1.1996 - acc: 0.5387 - val_loss: 0.9813 - val_acc: 0.7816\n",
      "Epoch 79/100\n",
      "78/78 [==============================] - 68s 874ms/step - loss: 1.2063 - acc: 0.5359 - val_loss: 1.0325 - val_acc: 0.7573\n",
      "Epoch 80/100\n",
      "78/78 [==============================] - 68s 869ms/step - loss: 1.1942 - acc: 0.5368 - val_loss: 1.0928 - val_acc: 0.7524\n",
      "Epoch 81/100\n",
      "78/78 [==============================] - 68s 869ms/step - loss: 1.2155 - acc: 0.5211 - val_loss: 0.9817 - val_acc: 0.7573\n",
      "Epoch 82/100\n",
      "78/78 [==============================] - 68s 872ms/step - loss: 1.1899 - acc: 0.5415 - val_loss: 0.9832 - val_acc: 0.7670\n",
      "Epoch 83/100\n",
      "78/78 [==============================] - 68s 869ms/step - loss: 1.1968 - acc: 0.5401 - val_loss: 1.0430 - val_acc: 0.7718\n",
      "Epoch 84/100\n",
      "78/78 [==============================] - 69s 879ms/step - loss: 1.1976 - acc: 0.5397 - val_loss: 1.0375 - val_acc: 0.7524\n",
      "Epoch 85/100\n",
      "78/78 [==============================] - 69s 888ms/step - loss: 1.1877 - acc: 0.5425 - val_loss: 1.0695 - val_acc: 0.7136\n",
      "Epoch 86/100\n",
      "78/78 [==============================] - 67s 861ms/step - loss: 1.2193 - acc: 0.5305 - val_loss: 1.0828 - val_acc: 0.7282\n",
      "Epoch 87/100\n",
      "78/78 [==============================] - 68s 870ms/step - loss: 1.2063 - acc: 0.5278 - val_loss: 1.0295 - val_acc: 0.7767\n",
      "Epoch 88/100\n",
      "78/78 [==============================] - 68s 870ms/step - loss: 1.1800 - acc: 0.5461 - val_loss: 1.1522 - val_acc: 0.6796\n",
      "Epoch 89/100\n",
      "78/78 [==============================] - 68s 870ms/step - loss: 1.2083 - acc: 0.5311 - val_loss: 0.9074 - val_acc: 0.6990\n",
      "Epoch 90/100\n",
      "78/78 [==============================] - 68s 870ms/step - loss: 1.2103 - acc: 0.5273 - val_loss: 1.0394 - val_acc: 0.7379\n",
      "Epoch 91/100\n",
      "78/78 [==============================] - 69s 880ms/step - loss: 1.2108 - acc: 0.5300 - val_loss: 0.9700 - val_acc: 0.7670\n",
      "Epoch 92/100\n",
      "78/78 [==============================] - 69s 889ms/step - loss: 1.1844 - acc: 0.5443 - val_loss: 1.1460 - val_acc: 0.6796\n",
      "Epoch 93/100\n",
      "78/78 [==============================] - 68s 868ms/step - loss: 1.2093 - acc: 0.5422 - val_loss: 1.0285 - val_acc: 0.7670\n",
      "Epoch 94/100\n",
      "78/78 [==============================] - 68s 870ms/step - loss: 1.1916 - acc: 0.5335 - val_loss: 0.9963 - val_acc: 0.7573\n",
      "Epoch 95/100\n",
      "78/78 [==============================] - 68s 870ms/step - loss: 1.2122 - acc: 0.5278 - val_loss: 1.0558 - val_acc: 0.7330\n",
      "Epoch 96/100\n",
      "78/78 [==============================] - 68s 870ms/step - loss: 1.2120 - acc: 0.5357 - val_loss: 1.1049 - val_acc: 0.6699\n",
      "Epoch 97/100\n",
      "78/78 [==============================] - 68s 870ms/step - loss: 1.1877 - acc: 0.5353 - val_loss: 1.0583 - val_acc: 0.7670\n",
      "Epoch 98/100\n",
      "78/78 [==============================] - 68s 873ms/step - loss: 1.2045 - acc: 0.5245 - val_loss: 0.9735 - val_acc: 0.7427\n",
      "Epoch 99/100\n",
      "78/78 [==============================] - 69s 879ms/step - loss: 1.1919 - acc: 0.5349 - val_loss: 1.0068 - val_acc: 0.7524\n",
      "Epoch 100/100\n",
      "78/78 [==============================] - 69s 888ms/step - loss: 1.1929 - acc: 0.5367 - val_loss: 1.1172 - val_acc: 0.7039\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "      epochs=100,\n",
    "      callbacks=[model_checkpoint],\n",
    "      validation_data=test_generator,\n",
    "      validation_steps=validation_generator.samples//validation_generator.batch_size,\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "78/78 [==============================] - 89s 1s/step - loss: 2.8143 - acc: 0.2384 - val_loss: 1.7667 - val_acc: 0.1748\n",
      "Epoch 2/150\n",
      "78/78 [==============================] - 69s 891ms/step - loss: 2.1014 - acc: 0.3396 - val_loss: 1.5484 - val_acc: 0.3835\n",
      "Epoch 3/150\n",
      "78/78 [==============================] - 69s 883ms/step - loss: 1.6722 - acc: 0.4357 - val_loss: 1.4311 - val_acc: 0.3883\n",
      "Epoch 4/150\n",
      "78/78 [==============================] - 68s 871ms/step - loss: 1.4058 - acc: 0.5028 - val_loss: 1.3222 - val_acc: 0.6893\n",
      "Epoch 5/150\n",
      "78/78 [==============================] - 69s 880ms/step - loss: 1.3167 - acc: 0.5231 - val_loss: 1.1293 - val_acc: 0.7184\n",
      "Epoch 6/150\n",
      "78/78 [==============================] - 68s 872ms/step - loss: 1.2730 - acc: 0.5202 - val_loss: 1.0838 - val_acc: 0.7282\n",
      "Epoch 7/150\n",
      "78/78 [==============================] - 69s 879ms/step - loss: 1.2671 - acc: 0.5183 - val_loss: 1.1154 - val_acc: 0.7184\n",
      "Epoch 8/150\n",
      "78/78 [==============================] - 69s 883ms/step - loss: 1.2421 - acc: 0.5231 - val_loss: 1.2382 - val_acc: 0.6553\n",
      "Epoch 9/150\n",
      "78/78 [==============================] - 68s 876ms/step - loss: 1.2517 - acc: 0.5169 - val_loss: 1.1215 - val_acc: 0.7233\n",
      "Epoch 10/150\n",
      "78/78 [==============================] - 68s 874ms/step - loss: 1.2465 - acc: 0.5156 - val_loss: 1.0627 - val_acc: 0.7379\n",
      "Epoch 11/150\n",
      "78/78 [==============================] - 68s 874ms/step - loss: 1.2530 - acc: 0.5204 - val_loss: 1.0381 - val_acc: 0.7233\n",
      "Epoch 12/150\n",
      "78/78 [==============================] - 69s 888ms/step - loss: 1.2404 - acc: 0.5235 - val_loss: 1.0817 - val_acc: 0.6942\n",
      "Epoch 13/150\n",
      "78/78 [==============================] - 69s 888ms/step - loss: 1.2185 - acc: 0.5383 - val_loss: 1.0493 - val_acc: 0.7427\n",
      "Epoch 14/150\n",
      "78/78 [==============================] - 67s 864ms/step - loss: 1.2283 - acc: 0.5319 - val_loss: 1.1271 - val_acc: 0.7233\n",
      "Epoch 15/150\n",
      "78/78 [==============================] - 70s 896ms/step - loss: 1.2243 - acc: 0.5276 - val_loss: 1.1046 - val_acc: 0.7427\n",
      "Epoch 16/150\n",
      "78/78 [==============================] - 69s 880ms/step - loss: 1.2167 - acc: 0.5288 - val_loss: 1.0851 - val_acc: 0.7330\n",
      "Epoch 17/150\n",
      "78/78 [==============================] - 68s 872ms/step - loss: 1.2213 - acc: 0.5346 - val_loss: 1.0181 - val_acc: 0.7379\n",
      "Epoch 18/150\n",
      "78/78 [==============================] - 69s 882ms/step - loss: 1.2291 - acc: 0.5274 - val_loss: 1.0986 - val_acc: 0.7670\n",
      "Epoch 19/150\n",
      "78/78 [==============================] - 68s 871ms/step - loss: 1.2039 - acc: 0.5349 - val_loss: 1.1107 - val_acc: 0.6408\n",
      "Epoch 20/150\n",
      "78/78 [==============================] - 69s 882ms/step - loss: 1.2219 - acc: 0.5305 - val_loss: 0.9741 - val_acc: 0.7330\n",
      "Epoch 21/150\n",
      "78/78 [==============================] - 69s 883ms/step - loss: 1.2129 - acc: 0.5391 - val_loss: 1.0510 - val_acc: 0.7573\n",
      "Epoch 22/150\n",
      "78/78 [==============================] - 70s 896ms/step - loss: 1.2152 - acc: 0.5351 - val_loss: 0.9991 - val_acc: 0.7184\n",
      "Epoch 23/150\n",
      "45/78 [================>.............] - ETA: 26s - loss: 1.2280 - acc: 0.5247"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f9a9df4b751c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m       \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m       verbose=1)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2230\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2232\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "      epochs=150,\n",
    "      callbacks=[model_checkpoint],\n",
    "      validation_data=test_generator,\n",
    "      validation_steps=validation_generator.samples//validation_generator.batch_size,\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    " \n",
    "epochs = range(len(acc))\n",
    " \n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    " \n",
    "plt.figure()\n",
    " \n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a compiled model identical to the previous one\n",
    "loaded_model = load_model(model_check_point_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Confution Matrix and Classification Report\n",
    "# Y_pred = loaded_model.predict_generator(validation_generator, validation_generator.samples // validation_generator.batch_size + 1)\n",
    "# y_pred = np.argmax(Y_pred, axis=1)\n",
    "# print('Confusion Matrix')\n",
    "# print(confusion_matrix(validation_generator.classes, y_pred))\n",
    "# print('Classification Report')\n",
    "# target_names = ['non-dr', 'mild-npdr', 'moderate-npdr', 'severe-npdr', 'pdr']\n",
    "# print(classification_report(validation_generator.classes, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confution Matrix and Classification Report for testset\n",
    "Y_pred_for_test = loaded_model.predict_generator(test_generator, test_generator.samples // test_generator.batch_size)\n",
    "Y_pred_for_test = np.argmax(Y_pred_for_test, axis=1)\n",
    "print('Confusion Matrix for testset')\n",
    "print(confusion_matrix(test_generator.classes, Y_pred_for_test))\n",
    "print('Classification Report')\n",
    "target_names = ['non-dr', 'mild-npdr', 'moderate-npdr', 'severe-npdr', 'pdr']\n",
    "print(classification_report(test_generator.classes, Y_pred_for_test, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate roc curve\n",
    "n_classes = 5\n",
    "\n",
    "# Plot linewidth.\n",
    "lw = 2\n",
    "\n",
    "#convert original class labels to the one-hot-encoding\n",
    "y_test = np_utils.to_categorical(test_generator.classes, 5)\n",
    "\n",
    "Y_test_pred = loaded_model.predict_generator(test_generator, test_generator.samples // test_generator.batch_size)\n",
    "Y_test_predicted = np.argmax(Y_test_pred, axis=1)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], Y_test_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), Y_test_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all ROC curves\n",
    "plt.figure(1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in view of the upper left corner.\n",
    "plt.figure(2)\n",
    "plt.xlim(0, 0.2)\n",
    "plt.ylim(0.8, 1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
