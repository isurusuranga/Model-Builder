{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP with automatic validation set\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    # return purity\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score_per_cluster(data, N):\n",
    "    return (data['y'].value_counts().max() / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pima indians dataset\n",
    "#dataset = numpy.loadtxt(\"D:/Neural_nets_course_Dataset/pima_indians_diabetes_data.csv\", delimiter=\",\")\n",
    "dataset = pd.read_csv('E:/Academic/Neural Networks/Assignment_2/iris.txt', delimiter=',', names = ['a','b','c','d','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     a    b    c    d  y\n",
       "0  5.1  3.5  1.4  0.2  1\n",
       "1  4.9  3.0  1.4  0.2  1\n",
       "2  4.6  3.1  1.5  0.2  1\n",
       "3  5.0  3.6  1.4  0.2  1\n",
       "4  4.6  3.4  1.4  0.3  1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,0:4].values\n",
    "Y = dataset.iloc[:,4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix = np.array([[0.80272936, 0.53990134, 0.24938941, 0.04401186], \n",
    "                         [0.70353431, 0.32379285, 0.59497289, 0.2149533], \n",
    "                         [0.62375013, 0.34441463, 0.17431373, 0.67965364]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2-normalize the samples (rows).\n",
    "X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "X_normalized = np.array(X_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.981698751449585 seconds ---\n"
     ]
    }
   ],
   "source": [
    "cluster_indexes = []\n",
    "alpha = 0.3\n",
    "ephsilon = 0.32\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(500):\n",
    "    cluster_indexes = []\n",
    "    for index,iterator in enumerate(X_normalized):\n",
    "        # to keep weighted summation for inputs\n",
    "        out_neuron_vector = np.dot(weight_matrix, X_normalized[index])\n",
    "        # to keep updated weighted summation including lateral inhibitory connections \n",
    "        out_neuron_vector_updated = np.zeros(out_neuron_vector.shape)\n",
    "        # to keep lateral inhibitory connections summation for each output index\n",
    "        lateral_inhibit_summation = np.zeros(out_neuron_vector.shape)\n",
    "        for idx, itr in enumerate(out_neuron_vector):\n",
    "            indexes_for_sum = [i for i in range(np.shape(out_neuron_vector)[0]) if i not in [idx]]\n",
    "            lateral_inhibit_summation[idx] = sum(out_neuron_vector[i] for i in indexes_for_sum)\n",
    "        # update after including lateral inhibitory connections\n",
    "        out_neuron_vector_updated = np.add(out_neuron_vector, (-ephsilon * lateral_inhibit_summation))\n",
    "        max_index = np.argmax(out_neuron_vector_updated)\n",
    "        # add cluster value for each tuple\n",
    "        cluster_indexes.append(max_index + 1)\n",
    "        # retrieve Wk vector\n",
    "        Wk = weight_matrix[max_index,:]\n",
    "        # update the Wk vector\n",
    "        pwx = alpha * np.subtract(X_normalized[index], Wk)\n",
    "        Wk = np.add(Wk, pwx)\n",
    "        # normalize Wk\n",
    "        euclidean_norm = np.sqrt(sum(Wk**2))\n",
    "        Wk = Wk / euclidean_norm\n",
    "        # add Wk to original weight matrix location\n",
    "        weight_matrix[max_index,:] = Wk\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2], dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(cluster_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity score for cluster 1:  0.3333333333333333\n",
      "Purity score for cluster 2:  0.3333333333333333\n",
      "Purity score for cluster 3:  0.0\n",
      "Total purity score:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "temp_dataset = dataset\n",
    "temp_dataset['cluster'] = cluster_indexes\n",
    "\n",
    "purity_score_cluster1 = 0.0\n",
    "purity_score_cluster2 = 0.0\n",
    "purity_score_cluster3 = 0.0\n",
    "\n",
    "#purity score for cluster 1\n",
    "cluster1_dataset = temp_dataset[temp_dataset['cluster'] == 1]\n",
    "if len(cluster1_dataset) != 0:\n",
    "    purity_score_cluster1 = purity_score_per_cluster(cluster1_dataset, len(dataset))\n",
    "\n",
    "#purity score for cluster 2\n",
    "cluster2_dataset = temp_dataset[temp_dataset['cluster'] == 2]\n",
    "if len(cluster2_dataset) != 0:\n",
    "    purity_score_cluster2 = purity_score_per_cluster(cluster2_dataset, len(dataset))\n",
    "\n",
    "#purity score for cluster 3\n",
    "cluster3_dataset = temp_dataset[temp_dataset['cluster'] == 3]\n",
    "if len(cluster3_dataset) != 0:\n",
    "    purity_score_cluster3 = purity_score_per_cluster(cluster3_dataset, len(dataset))\n",
    "\n",
    "print('Purity score for cluster 1: ', purity_score_cluster1)\n",
    "print('Purity score for cluster 2: ', purity_score_cluster2)\n",
    "print('Purity score for cluster 3: ', purity_score_cluster3)\n",
    "print('Total purity score: ', purity_score(Y, cluster_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.array([1, 2 , 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.zeros(out.shape)\n",
    "for index, it in enumerate(out):\n",
    "    if index == 0:\n",
    "        arr[index] = out[index + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inn = np.zeros(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_neuron_vector = np.array([4, 1 , 2])\n",
    "# to keep lateral inhibitory connections summation for each output index\n",
    "lateral_inhibit_summation = np.zeros(out_neuron_vector.shape)\n",
    "for idx, itr in enumerate(out_neuron_vector):\n",
    "    indexes_for_sum = [i for i in range(np.shape(out_neuron_vector)[0]) if i not in [idx]]\n",
    "    lateral_inhibit_summation[idx] = sum(out_neuron_vector[i] for i in indexes_for_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 6., 5.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lateral_inhibit_summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
