{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# MLP with automatic validation set\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    # return purity\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score_per_cluster(data, N):\n",
    "    return (data['y'].value_counts().max() / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "# seed = 6\n",
    "# np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pima indians dataset\n",
    "#dataset = numpy.loadtxt(\"D:/Neural_nets_course_Dataset/pima_indians_diabetes_data.csv\", delimiter=\",\")\n",
    "dataset = pd.read_csv('E:/Academic/Neural Networks/Assignment_2/iris.txt', delimiter=',', names = ['a','b','c','d','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     a    b    c    d  y\n",
       "0  5.1  3.5  1.4  0.2  1\n",
       "1  4.9  3.0  1.4  0.2  1\n",
       "2  4.6  3.1  1.5  0.2  1\n",
       "3  5.0  3.6  1.4  0.2  1\n",
       "4  4.6  3.4  1.4  0.3  1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,0:4].values\n",
    "Y = dataset.iloc[:,4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilaize to very small random values\n",
    "weight_matrix = np.random.rand(3, 4)\n",
    "weight_matrix = preprocessing.normalize(weight_matrix, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57747178, 0.47902018, 0.48758658, 0.44645866],\n",
       "       [0.76267533, 0.11942354, 0.48981408, 0.40515   ],\n",
       "       [0.62375013, 0.34441463, 0.17431373, 0.67965364]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2-normalize the samples (rows).\n",
    "X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "X_normalized = np.array(X_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_indexes = []\n",
    "alpha = 0.3\n",
    "\n",
    "for i in range(500):\n",
    "    cluster_indexes = []\n",
    "    for index,iterator in enumerate(X_normalized):\n",
    "        out_neuron_vector = np.dot(weight_matrix, X_normalized[index])\n",
    "        max_index = np.argmax(out_neuron_vector)\n",
    "        # add cluster value for each tuple\n",
    "        cluster_indexes.append(max_index + 1)\n",
    "        # retrieve Wk vector\n",
    "        Wk = weight_matrix[max_index,:]\n",
    "        # update the Wk vector\n",
    "        pwx = alpha * np.subtract(X_normalized[index], Wk)\n",
    "        Wk = np.add(Wk, pwx)\n",
    "        # normalize Wk\n",
    "        euclidean_norm = np.sqrt(sum(Wk**2))\n",
    "        Wk = Wk / euclidean_norm\n",
    "        # add Wk to original weight matrix location\n",
    "        weight_matrix[max_index,:] = Wk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(cluster_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cluster_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity score for cluster 1:  0.3333333333333333\n",
      "Purity score for cluster 2:  0.3333333333333333\n",
      "Purity score for cluster 3:  0.0\n",
      "Total purity score:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "temp_dataset = dataset\n",
    "temp_dataset['cluster'] = cluster_indexes\n",
    "\n",
    "purity_score_cluster1 = 0.0\n",
    "purity_score_cluster2 = 0.0\n",
    "purity_score_cluster3 = 0.0\n",
    "\n",
    "#purity score for cluster 1\n",
    "cluster1_dataset = temp_dataset[temp_dataset['cluster'] == 1]\n",
    "if len(cluster1_dataset) != 0:\n",
    "    purity_score_cluster1 = purity_score_per_cluster(cluster1_dataset, len(dataset))\n",
    "\n",
    "#purity score for cluster 2\n",
    "cluster2_dataset = temp_dataset[temp_dataset['cluster'] == 2]\n",
    "if len(cluster2_dataset) != 0:\n",
    "    purity_score_cluster2 = purity_score_per_cluster(cluster2_dataset, len(dataset))\n",
    "\n",
    "#purity score for cluster 3\n",
    "cluster3_dataset = temp_dataset[temp_dataset['cluster'] == 3]\n",
    "if len(cluster3_dataset) != 0:\n",
    "    purity_score_cluster3 = purity_score_per_cluster(cluster3_dataset, len(dataset))\n",
    "\n",
    "print('Purity score for cluster 1: ', purity_score_cluster1)\n",
    "print('Purity score for cluster 2: ', purity_score_cluster2)\n",
    "print('Purity score for cluster 3: ', purity_score_cluster3)\n",
    "print('Total purity score: ', purity_score(Y, cluster_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purity_score(Y, cluster_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a=np.array([1,2,3,4])\n",
    "b=np.argmax(a)\n",
    "print(a[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_mat = np.random.randn(3, 4)\n",
    "a = [[1,2,3,4], [3, 5, 6, 8]]\n",
    "#out_vector = np.dot(weight_mat, a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.34329247,  0.7205635 , -0.02352852,  0.60198416],\n",
       "       [-0.19645069,  0.0851739 ,  0.94029928,  0.26455584],\n",
       "       [-0.2471944 ,  0.8458623 , -0.01673924, -0.47236819]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4], [3, 5, 6, 8]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.34329247 -1.2794365  -3.02352852 -3.39801584]\n",
      "[-3.34329247 -4.2794365  -6.02352852 -7.39801584]\n"
     ]
    }
   ],
   "source": [
    "for index, x in enumerate(a):\n",
    "    print(np.subtract(weight_mat[0], a[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.43518562  3.85301831 -0.49516027]\n",
      "[ 7.24764227  7.59475983 -0.39165264]\n"
     ]
    }
   ],
   "source": [
    "for index, x in enumerate(a):\n",
    "    print(np.dot(weight_mat, np.array(a[index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.56079862,  1.17710424, -0.03843592,  0.98339438],\n",
       "       [-0.30294356,  0.11643074,  1.40203911,  0.37755632],\n",
       "       [-0.12474052,  0.42684341, -0.00844704, -0.23836888]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_mat = preprocessing.normalize(weight_mat, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.34329247,  0.7205635 , -0.02352852,  0.60198416],\n",
       "       [-0.20361642,  0.0782562 ,  0.94234777,  0.25376564],\n",
       "       [-0.2471944 ,  0.8458623 , -0.01673924, -0.47236819]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_normalized = preprocessing.normalize(a, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18257419, 0.36514837, 0.54772256, 0.73029674],\n",
       "       [0.25916053, 0.43193421, 0.51832106, 0.69109474]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36514837, 0.73029674, 1.09544512, 1.46059349],\n",
       "       [0.51832106, 0.86386843, 1.03664211, 1.38218948]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_normalized * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18257419 0.36514837 0.54772256 0.73029674]\n",
      "out_neuron_vector:\n",
      "[ 0.62717622  0.69286937 -0.09040348]\n",
      "index: 2\n",
      "extracted mat loc: [-0.20361642  0.0782562   0.94234777  0.25376564]\n",
      "Wk:\n",
      "[-0.20036467  0.08137292  0.94126789  0.25932065]\n",
      "updated weight matrix:\n",
      "[[-0.34329247  0.7205635  -0.02352852  0.60198416]\n",
      " [-0.20036467  0.08137292  0.94126789  0.25932065]\n",
      " [-0.2471944   0.8458623  -0.01673924 -0.47236819]]\n",
      "[0.25916053 0.43193421 0.51832106 0.69109474]\n",
      "out_neuron_vector:\n",
      "[ 0.62610093  0.65031523 -0.03383363]\n",
      "index: 2\n",
      "extracted mat loc: [-0.20036467  0.08137292  0.94126789  0.25932065]\n",
      "Wk:\n",
      "[-0.19645069  0.0851739   0.94029928  0.26455584]\n",
      "updated weight matrix:\n",
      "[[-0.34329247  0.7205635  -0.02352852  0.60198416]\n",
      " [-0.19645069  0.0851739   0.94029928  0.26455584]\n",
      " [-0.2471944   0.8458623  -0.01673924 -0.47236819]]\n"
     ]
    }
   ],
   "source": [
    "for index,iterator in enumerate(a_normalized):\n",
    "    print(a_normalized[index])\n",
    "    out_neuron_vector = np.dot(weight_mat, a_normalized[index])\n",
    "    print('out_neuron_vector:')\n",
    "    print(out_neuron_vector)\n",
    "    b = np.argmax(out_neuron_vector)\n",
    "    print('index:', b + 1)\n",
    "    print('extracted mat loc:', weight_mat[b,:])\n",
    "    pxw = np.subtract(a_normalized[index], weight_mat[b,:])\n",
    "    Wk = np.add(weight_mat[b,:], (0.01 * pxw))\n",
    "    \n",
    "    euclidean_norm = np.sqrt(sum(Wk**2))\n",
    "    Wk = Wk / euclidean_norm\n",
    "    print('Wk:')\n",
    "    print(Wk)\n",
    "    weight_mat[b,:] = Wk\n",
    "    print('updated weight matrix:')\n",
    "    print(weight_mat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1. 2. 3.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-ded33ab89726>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[0;32m   1410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m     X = check_array(X, sparse_format, copy=copy,\n\u001b[1;32m-> 1412\u001b[1;33m                     estimator='the normalize function', dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[0;32m   1413\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[1;31m# To ensure that array flags are maintained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1. 2. 3.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "preprocessing.normalize([1, 2, 3], norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance from x to y:  4.69041575982343\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# Example points in 3-dimensional space...\n",
    "x = (5, 6, 7)\n",
    "y = (8, 9, 9)\n",
    "distance = math.sqrt(sum([(a - b) ** 2 for a, b in zip(x, y)]))\n",
    "print(\"Euclidean distance from x to y: \",distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.69041576, 13.3041347 , 11.5758369 ])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "x = [[5, 6, 7], [1, 1, 1], [2, 2, 2]]\n",
    "y = [8, 9, 9]\n",
    "#np.sqrt(np.sum(np.square(np.array(x)-np.array(y).reshape(1,3)),axis=0))\n",
    "np.linalg.norm(np.array(x)-np.array(y), axis=1, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
